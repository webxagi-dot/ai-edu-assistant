import streamlit as st
from langchain_community.chat_models import ChatZhipuAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
import os
import time
import re
import tempfile
from collections import Counter
import networkx as nx

# ===== å°è¯•å¯¼å…¥ pyvisï¼Œå¦‚æœå¤±è´¥åˆ™åç»­ä½¿ç”¨ matplotlib =====
try:
    from pyvis.network import Network
    pyvis_available = True
except ImportError:
    pyvis_available = False
    import matplotlib.pyplot as plt

# ===== é¡µé¢é…ç½® =====
st.set_page_config(
    page_title="AI æ•™è‚²å¹³å°",
    page_icon="ğŸ“",
    layout="wide"
)

# ===== ç§»åŠ¨ç«¯é€‚é… CSS =====
st.markdown("""
<style>
    @media (max-width: 768px) {
        .main .block-container {
            padding-left: 1rem;
            padding-right: 1rem;
        }
        .stChatMessage {
            margin-bottom: 0.5rem;
        }
        .stTextArea textarea {
            font-size: 16px;
        }
    }
    .voice-btn {
        margin-right: 10px;
    }
</style>
""", unsafe_allow_html=True)

# ===== åˆå§‹åŒ–ä¼šè¯çŠ¶æ€ =====
if "vectorstores" not in st.session_state:
    st.session_state.vectorstores = {}          # æ•™æå -> vectorstore
if "current_textbook" not in st.session_state:
    st.session_state.current_textbook = None
if "current_textbook_content" not in st.session_state:
    st.session_state.current_textbook_content = ""
if "qa_history" not in st.session_state:
    st.session_state.qa_history = []            # ç”¨äºå­¦æƒ…åˆ†æ
if "messages" not in st.session_state:
    st.session_state.messages = []              # èŠå¤©å†å²

# ===== ä¾§è¾¹æ å¯¼èˆª =====
st.sidebar.title("ğŸ“ AI æ•™è‚²å¹³å°")
page = st.sidebar.radio(
    "é€‰æ‹©åŠŸèƒ½",
    ["ğŸ“š æ™ºèƒ½åŠ©æ•™", "ğŸ“ ä½œæ–‡æ‰¹æ”¹", "âœï¸ ä¹ é¢˜ç”Ÿæˆ", "ğŸ“Š å­¦æƒ…åˆ†æ", "ğŸ§  çŸ¥è¯†å›¾è°±"]
)

st.sidebar.markdown("---")
st.sidebar.markdown("### âš™ï¸ å…¨å±€è®¾ç½®")

# API Key è¾“å…¥
default_key = os.getenv("ZHIPU_API_KEY", "1d9ee499e7bb413aaabe015a87b7773c.3UrwmR1C6Ew1gfDy")
api_key = st.sidebar.text_input(
    "æ™ºè°±AI API Key",
    type="password",
    value=default_key
)

# æ¸©åº¦è°ƒèŠ‚
temperature = st.sidebar.slider(
    "å›ç­”æ¸©åº¦",
    min_value=0.0,
    max_value=1.0,
    value=0.3,
    step=0.1
)

# ===== åˆå§‹åŒ– AI =====
@st.cache_resource
def get_llm():
    return ChatZhipuAI(
        api_key=api_key,
        model="glm-4-flash",
        temperature=temperature
    )

if api_key:
    llm = get_llm()

# ===== åŠŸèƒ½1: æ™ºèƒ½åŠ©æ•™ =====
if page == "ğŸ“š æ™ºèƒ½åŠ©æ•™":
    st.title("ğŸ“š æ™ºèƒ½åŠ©æ•™")
    st.markdown("---")

    # ---- æ•™æç®¡ç†åŒºåŸŸ ----
    col1, col2 = st.columns([3, 1])
    with col1:
        uploaded_file = st.file_uploader("ä¸Šä¼ æ–°æ•™æï¼ˆ.txtï¼‰", type=['txt'], key="upload")
    with col2:
        st.write("")  # å‚ç›´å ä½
        st.write("")
        if st.button("ğŸ“– ä½¿ç”¨å½“å‰æ•™æ"):
            pass  # ä¸‹æ‹‰æ¡†ä¼šå¤„ç†

    # å·²æœ‰æ•™æé€‰æ‹©
    if st.session_state.vectorstores:
        selected = st.selectbox(
            "é€‰æ‹©å½“å‰æ•™æ",
            list(st.session_state.vectorstores.keys()),
            index=0
        )
        if selected != st.session_state.current_textbook:
            st.session_state.current_textbook = selected
            st.rerun()

    # å¤„ç†æ–°ä¸Šä¼ æ•™æ
    if uploaded_file is not None:
        # è¯»å–æ–‡ä»¶å†…å®¹
        try:
            textbook = uploaded_file.getvalue().decode("utf-8")
        except UnicodeDecodeError:
            try:
                textbook = uploaded_file.getvalue().decode("gbk")
            except UnicodeDecodeError:
                st.error("âŒ æ–‡ä»¶ç¼–ç é”™è¯¯ï¼šè¯·ç¡®ä¿ä¸Šä¼ çš„æ–‡ä»¶æ˜¯ UTF-8 æˆ– GBK ç¼–ç çš„çº¯æ–‡æœ¬æ–‡ä»¶ï¼ˆ.txtï¼‰")
                st.stop()

        textbook_name = uploaded_file.name
        if textbook_name not in st.session_state.vectorstores:
            with st.spinner(f"æ­£åœ¨å¤„ç†æ•™æã€Š{textbook_name}ã€‹..."):
                # åˆ†å‰²æ–‡æœ¬
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=200,
                    chunk_overlap=50,
                    separators=["\n\n", "\n", "ã€‚", "ï¼›"]
                )
                texts = text_splitter.split_text(textbook)

                # åŠ è½½ embeddings
                embeddings = HuggingFaceEmbeddings(
                    model_name="sentence-transformers/all-MiniLM-L6-v2",
                    model_kwargs={'device': 'cpu'}
                )

                # åˆ›å»ºå‘é‡æ•°æ®åº“
                vectorstore = Chroma.from_texts(
                    texts=texts,
                    embedding=embeddings,
                    persist_directory=f"./chroma_db_{textbook_name}"
                )
                st.session_state.vectorstores[textbook_name] = vectorstore
                st.session_state.current_textbook = textbook_name
                st.session_state.current_textbook_content = textbook
                st.success(f"âœ… æ•™æã€Š{textbook_name}ã€‹å·²æ·»åŠ ")
                st.rerun()
        else:
            st.info(f"æ•™æã€Š{textbook_name}ã€‹å·²å­˜åœ¨")
            st.session_state.current_textbook = textbook_name
            st.session_state.current_textbook_content = textbook
            st.rerun()

    # å¦‚æœæ²¡æœ‰é€‰æ‹©æ•™æï¼Œæç¤º
    if not st.session_state.current_textbook:
        st.info("è¯·å…ˆä¸Šä¼ æˆ–é€‰æ‹©ä¸€æœ¬æ•™æã€‚")
        st.stop()

    # ---- å¯¹è¯ç•Œé¢ ----
    vectorstore = st.session_state.vectorstores[st.session_state.current_textbook]
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    # æç¤ºè¯æ¨¡æ¿
    template = """ä½ æ˜¯ä¸€ä¸ªè€å¿ƒçš„è€å¸ˆã€‚è¯·åŸºäºä»¥ä¸‹æ•™æå†…å®¹å›ç­”å­¦ç”Ÿçš„é—®é¢˜ã€‚

æ•™æå†…å®¹ï¼š
{context}

å­¦ç”Ÿé—®é¢˜ï¼š{question}

è¦æ±‚ï¼š
1. å¦‚æœæ•™æä¸­æœ‰ç›¸å…³å†…å®¹ï¼Œè¯·åŸºäºæ•™æå‡†ç¡®å›ç­”
2. å¦‚æœæ•™æä¸­æ²¡æœ‰ï¼Œè¯·è¯´"æ•™æä¸­æ²¡æœ‰æ‰¾åˆ°ï¼Œä¸è¿‡æ ¹æ®æˆ‘çš„ç†è§£ï¼š"
3. ç”¨ç®€å•æ˜“æ‡‚çš„è¯­è¨€ï¼Œå¯ä»¥ä¸¾ä¾‹è¯´æ˜

ä½ çš„å›ç­”ï¼š"""
    prompt_template = PromptTemplate.from_template(template)

    def format_docs(docs):
        return "\n\n".join([doc.page_content for doc in docs])

    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt_template
        | llm
        | StrOutputParser()
    )

    # æ˜¾ç¤ºèŠå¤©å†å²
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # ---- è‡ªå®šä¹‰è¾“å…¥åŒºï¼ˆä¸å«è¯­éŸ³ï¼‰ ----
    col1, col2 = st.columns([6, 1])
    with col1:
        user_input = st.text_area("è¾“å…¥ä½ çš„é—®é¢˜", key="chat_input", height=100, label_visibility="collapsed")
    with col2:
        st.write("")
        st.write("")
        send_btn = st.button("ğŸ“¤ å‘é€", type="primary")

    # å¤„ç†å‘é€
    if send_btn and user_input:
        # ä¿å­˜åˆ°å†å²
        st.session_state.messages.append({"role": "user", "content": user_input})
        st.session_state.qa_history.append({
            "question": user_input,
            "answer": None,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        })

        with st.chat_message("assistant"):
            with st.spinner("æ€è€ƒä¸­..."):
                docs = retriever.invoke(user_input)
                answer = rag_chain.invoke(user_input)
                st.markdown(answer)
                with st.expander("ğŸ“– å‚è€ƒæ•™æ"):
                    for i, doc in enumerate(docs):
                        st.text(f"{i+1}. {doc.page_content[:100]}...")

        # æ›´æ–°å†å²ä¸­çš„ç­”æ¡ˆ
        st.session_state.qa_history[-1]["answer"] = answer
        st.session_state.messages.append({"role": "assistant", "content": answer})
        st.rerun()

# ===== åŠŸèƒ½2: ä½œæ–‡æ‰¹æ”¹ =====
elif page == "ğŸ“ ä½œæ–‡æ‰¹æ”¹":
    st.title("ğŸ“ ä½œæ–‡æ‰¹æ”¹åŠ©æ‰‹")
    st.markdown("---")

    if not api_key:
        st.error("è¯·å…ˆåœ¨ä¾§è¾¹æ è¾“å…¥æ™ºè°±AI API Key")
        st.stop()

    col1, col2 = st.columns([1, 1])
    with col1:
        topic = st.text_input("âœï¸ ä½œæ–‡é¢˜ç›®", placeholder="ä¾‹å¦‚ï¼šæˆ‘çš„æ¢¦æƒ³")
        grade_level = st.selectbox("ğŸ“Š å¹´çº§", ["å°å­¦", "åˆä¸­", "é«˜ä¸­", "å¤§å­¦"])
    with col2:
        word_count = st.number_input("ğŸ“ å­—æ•°è¦æ±‚", min_value=100, max_value=1000, value=500, step=50)
        style = st.selectbox("ğŸ“ æ–‡ä½“", ["è®°å™æ–‡", "è®®è®ºæ–‡", "è¯´æ˜æ–‡", "åº”ç”¨æ–‡"])

    essay = st.text_area("ğŸ“„ å­¦ç”Ÿä½œæ–‡", height=300, placeholder="åœ¨è¿™é‡Œç²˜è´´å­¦ç”Ÿçš„ä½œæ–‡...")

    if st.button("âœ¨ å¼€å§‹æ‰¹æ”¹", type="primary"):
        if not essay or not topic:
            st.error("è¯·å¡«å†™ä½œæ–‡é¢˜ç›®å’Œå†…å®¹")
        else:
            with st.spinner("AI æ­£åœ¨æ‰¹æ”¹ä¸­..."):
                grading_prompt = f"""ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„è¯­æ–‡è€å¸ˆã€‚è¯·å¯¹ä»¥ä¸‹ä½œæ–‡è¿›è¡Œæ‰¹æ”¹ã€‚

ä½œæ–‡é¢˜ç›®ï¼š{topic}
å¹´çº§ï¼š{grade_level}
å­—æ•°è¦æ±‚ï¼š{word_count}å­—
æ–‡ä½“ï¼š{style}

å­¦ç”Ÿä½œæ–‡ï¼š
{essay}

è¯·ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è¿›è¡Œæ‰¹æ”¹ï¼Œå¹¶ç»™å‡ºç™¾åˆ†åˆ¶æ€»åˆ†ï¼š

1. å†…å®¹ï¼ˆ30åˆ†ï¼‰ï¼šä¸»é¢˜æ˜ç¡®ã€å†…å®¹å……å®ã€è§‚ç‚¹æ¸…æ™°
2. ç»“æ„ï¼ˆ30åˆ†ï¼‰ï¼šå±‚æ¬¡åˆ†æ˜ã€è¿‡æ¸¡è‡ªç„¶ã€é€»è¾‘æ¸…æ™°
3. è¯­è¨€ï¼ˆ30åˆ†ï¼‰ï¼šè¡¨è¾¾å‡†ç¡®ã€è¯æ±‡ä¸°å¯Œã€å¥å¼å¤šæ ·
4. åˆ›æ„ï¼ˆ10åˆ†ï¼‰ï¼šæœ‰æ–°æ„ã€æœ‰ç‰¹è‰²

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š
ã€æ€»åˆ†ã€‘XXåˆ†
ã€å†…å®¹è¯„åˆ†ã€‘XXåˆ† è¯„è¯­ï¼š...
ã€ç»“æ„è¯„åˆ†ã€‘XXåˆ† è¯„è¯­ï¼š...
ã€è¯­è¨€è¯„åˆ†ã€‘XXåˆ† è¯„è¯­ï¼š...
ã€åˆ›æ„è¯„åˆ†ã€‘XXåˆ† è¯„è¯­ï¼š...
ã€è¯¦ç»†è¯„è¯­ã€‘...
ã€ä¿®æ”¹å»ºè®®ã€‘...
ã€ç¤ºèŒƒæ®µè½ã€‘...
"""
                response = llm.invoke(grading_prompt)
                st.success("âœ… æ‰¹æ”¹å®Œæˆï¼")
                result_text = response.content

                # ç®€å•æå–æ€»åˆ†æ˜¾ç¤º
                score_match = re.search(r'ã€æ€»åˆ†ã€‘(\d+)', result_text)
                if score_match:
                    total_score = int(score_match.group(1))
                    c1, c2, c3, c4 = st.columns(4)
                    with c1:
                        st.metric("æ€»åˆ†", f"{total_score}/100")
                    with c2:
                        st.metric("å†…å®¹", re.search(r'ã€å†…å®¹è¯„åˆ†ã€‘(\d+)', result_text).group(1) if re.search(r'ã€å†…å®¹è¯„åˆ†ã€‘(\d+)', result_text) else "?")
                    with c3:
                        st.metric("ç»“æ„", re.search(r'ã€ç»“æ„è¯„åˆ†ã€‘(\d+)', result_text).group(1) if re.search(r'ã€ç»“æ„è¯„åˆ†ã€‘(\d+)', result_text) else "?")
                    with c4:
                        st.metric("è¯­è¨€", re.search(r'ã€è¯­è¨€è¯„åˆ†ã€‘(\d+)', result_text).group(1) if re.search(r'ã€è¯­è¨€è¯„åˆ†ã€‘(\d+)', result_text) else "?")

                with st.expander("ğŸ“‹ è¯¦ç»†æ‰¹æ”¹ç»“æœ", expanded=True):
                    st.markdown(result_text)

# ===== åŠŸèƒ½3: ä¹ é¢˜ç”Ÿæˆ =====
elif page == "âœï¸ ä¹ é¢˜ç”Ÿæˆ":
    st.title("âœï¸ æ™ºèƒ½ä¹ é¢˜ç”Ÿæˆ")
    st.markdown("---")

    if not api_key:
        st.error("è¯·å…ˆåœ¨ä¾§è¾¹æ è¾“å…¥æ™ºè°±AI API Key")
        st.stop()

    col1, col2 = st.columns(2)
    with col1:
        subject = st.selectbox("ğŸ“š ç§‘ç›®", ["Pythonç¼–ç¨‹", "æ•°å­¦", "è‹±è¯­", "è¯­æ–‡", "ç‰©ç†"])
        topic = st.text_input("ğŸ¯ çŸ¥è¯†ç‚¹", placeholder="ä¾‹å¦‚ï¼šforå¾ªç¯ã€ä¸€å…ƒäºŒæ¬¡æ–¹ç¨‹ã€ä¸€èˆ¬ç°åœ¨æ—¶")
    with col2:
        difficulty = st.select_slider("ğŸ“Š éš¾åº¦", options=["å…¥é—¨", "ç®€å•", "ä¸­ç­‰", "å›°éš¾", "æŒ‘æˆ˜"])
        question_type = st.multiselect("ğŸ“ é¢˜å‹", ["é€‰æ‹©é¢˜", "å¡«ç©ºé¢˜", "ç®€ç­”é¢˜", "ç¼–ç¨‹é¢˜"], default=["é€‰æ‹©é¢˜"])

    count = st.number_input("ğŸ“‹ é¢˜ç›®æ•°é‡", min_value=1, max_value=10, value=3)

    if st.button("âœ¨ ç”Ÿæˆä¹ é¢˜", type="primary"):
        if not topic:
            st.error("è¯·è¾“å…¥çŸ¥è¯†ç‚¹")
        else:
            with st.spinner("AI æ­£åœ¨å‡ºé¢˜..."):
                exercise_prompt = f"""ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„{subject}è€å¸ˆã€‚è¯·æ ¹æ®ä»¥ä¸‹è¦æ±‚ç”Ÿæˆç»ƒä¹ é¢˜ã€‚

ç§‘ç›®ï¼š{subject}
çŸ¥è¯†ç‚¹ï¼š{topic}
éš¾åº¦ï¼š{difficulty}
é¢˜å‹ï¼š{', '.join(question_type)}
é¢˜ç›®æ•°é‡ï¼š{count}

è¦æ±‚ï¼š
1. é¢˜ç›®è¦è¦†ç›–çŸ¥è¯†ç‚¹çš„æ ¸å¿ƒå†…å®¹
2. éš¾åº¦è¦é€‚åˆ{difficulty}æ°´å¹³
3. é¢˜å‹è¦å¤šæ ·åŒ–
4. æä¾›å‚è€ƒç­”æ¡ˆå’Œè§£æ
5. é¢˜ç›®è¡¨è¿°è¦æ¸…æ™°å‡†ç¡®

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºæ¯ä¸ªé¢˜ç›®ï¼š
ã€é¢˜ç›®Xã€‘
é¢˜å‹ï¼š[é¢˜å‹]
é¢˜ç›®ï¼š[é¢˜ç›®å†…å®¹]
ç­”æ¡ˆï¼š[å‚è€ƒç­”æ¡ˆ]
è§£æï¼š[è¯¦ç»†è§£æ]
---
"""
                response = llm.invoke(exercise_prompt)
                st.success("âœ… ä¹ é¢˜ç”Ÿæˆå®Œæˆï¼")
                exercises = response.content.split("---")
                for i, ex in enumerate(exercises):
                    if ex.strip():
                        with st.expander(f"ğŸ“Œ ç¬¬{i+1}é¢˜", expanded=i==0):
                            st.markdown(ex)

# ===== åŠŸèƒ½4: å­¦æƒ…åˆ†æ =====
elif page == "ğŸ“Š å­¦æƒ…åˆ†æ":
    st.title("ğŸ“Š å­¦æƒ…åˆ†æ")
    st.markdown("---")

    if len(st.session_state.qa_history) == 0:
        st.info("æš‚æ— é—®ç­”è®°å½•ï¼Œè¯·å…ˆåœ¨æ™ºèƒ½åŠ©æ•™ä¸­æé—®ã€‚")
    else:
        st.subheader(f"æ€»æé—®æ•°ï¼š{len(st.session_state.qa_history)}")

        # å…³é”®è¯ç»Ÿè®¡
        try:
            import jieba
            all_questions = " ".join([item["question"] for item in st.session_state.qa_history])
            words = jieba.lcut(all_questions)
            stopwords = set(["çš„", "äº†", "æ˜¯", "åœ¨", "å’Œ", "æœ‰", "è¿™ä¸ª", "é‚£ä¸ª", "ä»€ä¹ˆ", "æ€ä¹ˆ", "å¦‚ä½•", "ä¸ºä»€ä¹ˆ", "å—", "å‘¢", "å§", "å•Š"])
            keywords = [w for w in words if len(w) > 1 and w not in stopwords]
            counter = Counter(keywords).most_common(10)

            st.subheader("ğŸ” é«˜é¢‘å…³é”®è¯")
            for word, count in counter:
                st.write(f"{word} : {count}æ¬¡")
        except ImportError:
            st.warning("æœªå®‰è£… jieba åˆ†è¯åº“ï¼Œæ— æ³•è¿›è¡Œå…³é”®è¯åˆ†æã€‚")

        # æœ€è¿‘é—®ç­”
        st.subheader("ğŸ“œ æœ€è¿‘é—®ç­”")
        for qa in st.session_state.qa_history[-10:]:
            with st.expander(f"Q: {qa['question'][:50]}..."):
                st.write(f"**æ—¶é—´**ï¼š{qa['timestamp']}")
                st.write(f"**A**: {qa['answer']}")

# ===== åŠŸèƒ½5: çŸ¥è¯†å›¾è°± =====
elif page == "ğŸ§  çŸ¥è¯†å›¾è°±":
    st.title("ğŸ§  çŸ¥è¯†ç‚¹å›¾è°±")
    st.markdown("---")

    if not st.session_state.current_textbook_content:
        st.info("è¯·å…ˆåœ¨æ™ºèƒ½åŠ©æ•™ä¸­ä¸Šä¼ ä¸€æœ¬æ•™æã€‚")
        st.stop()

    # æå–ç« èŠ‚æ ‡é¢˜ï¼ˆç®€å•æ­£åˆ™ï¼‰
    text = st.session_state.current_textbook_content
    chapters = re.findall(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« \s*([^\n]+)', text)
    sections = re.findall(r'\d+\.\d+\s+([^\n]+)', text)
    nodes = chapters + sections

    if len(nodes) == 0:
        st.warning("æœªèƒ½ä»æ•™æä¸­æå–å‡ºç« èŠ‚æ ‡é¢˜ï¼Œè¯·æ£€æŸ¥æ•™ææ ¼å¼ã€‚")
    else:
        st.subheader(f"å…±æå–åˆ° {len(nodes)} ä¸ªçŸ¥è¯†ç‚¹")

        # æ„å»ºç®€å•å›¾ï¼ˆç« èŠ‚ä¹‹é—´é¡ºåºè¿æ¥ï¼‰
        G = nx.DiGraph()
        for i, node in enumerate(nodes):
            G.add_node(node, label=node, size=20)
            if i > 0:
                G.add_edge(nodes[i-1], node)

        if pyvis_available:
            # ä½¿ç”¨ pyvis ç”Ÿæˆäº¤äº’å¼ HTML
            net = Network(height="600px", width="100%", bgcolor="#ffffff", font_color="black")
            net.from_nx(G)
            net.toggle_physics(False)
            with tempfile.NamedTemporaryFile(delete=False, suffix='.html') as tmp:
                net.save_graph(tmp.name)
                with open(tmp.name, 'r', encoding='utf-8') as f:
                    html_content = f.read()
            st.components.v1.html(html_content, height=600, scrolling=True)
        else:
            # ä½¿ç”¨ matplotlib ç”Ÿæˆé™æ€å›¾
            st.warning("âš ï¸ æœªå®‰è£… pyvis åº“ï¼Œå°†ä½¿ç”¨é™æ€å›¾æ˜¾ç¤ºã€‚å¦‚éœ€äº¤äº’å¼å›¾è°±ï¼Œè¯·è¿è¡Œ `pip install pyvis` å¹¶é‡å¯åº”ç”¨ã€‚")
            plt.figure(figsize=(10, 6))
            pos = nx.spring_layout(G, k=1, iterations=50)
            nx.draw(G, pos, with_labels=True, node_color='lightblue', edge_color='gray',
                    node_size=1500, font_size=8, arrows=True)
            plt.title("çŸ¥è¯†ç‚¹å›¾è°±ï¼ˆé™æ€ï¼‰")
            st.pyplot(plt)